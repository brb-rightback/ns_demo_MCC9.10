{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4dcdf-02b7-4624-abb3-d16f6793388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import uproot3 as uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import scipy.optimize\n",
    "from scipy.optimize import curve_fit\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2bd0a-6a3d-48b4-9594-d1f614467e3f",
   "metadata": {},
   "source": [
    "## Set the following to your file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce340992-8057-43e5-963d-693670ae6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Change these paths #########################\n",
    "# set this to switch between the \"old file\" produced with MCC9.10 and \n",
    "bnb_data_filename = \"\"\n",
    "new_bnb_MC_filename = \"\"\n",
    "\n",
    "numi_data_filename = \"\"\n",
    "old_numi_MC_filename = \"\"\n",
    "new_numi_MC_filename = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1dc5a5-f779-4613-b3f1-a68c52c33bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af7fe9-ab82-4535-89e6-0239bc5c30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for performing the guassian fit\n",
    "def gaus(x,a,x0,sigma, offset):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2)) + offset\n",
    "\n",
    "#helper function to get bin centers\n",
    "def get_bin_centers(x):\n",
    "    centers = []\n",
    "    for i in range(len(x)-1): centers.append( x[i] + (x[i+1]-x[i])/2 )\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89793e-8dc9-40f9-a44b-f25a9f82cd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9580387f-739a-43f5-abdb-b085e70c44a3",
   "metadata": {},
   "source": [
    "## Plot the merged beam peak for the BNB data\n",
    "No special steps, this can be done the same for the old and new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7261dd3-2251-4f05-b720-6896bde1f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the BNB data\n",
    "file = uproot.open(bnb_data_filename)\n",
    "pfeval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"evtTimeNS\",\"run\",\"subrun\",\"event\"], flatten=False)\n",
    "bdt_df = file[\"wcpselection\"][\"T_BDTvars\"].pandas.df([\"numu_score\"], flatten=False)\n",
    "df_bnb_data = pd.concat([pfeval_df,bdt_df], axis=1, sort=False)\n",
    "del pfeval_df\n",
    "del bdt_df\n",
    "print(df_bnb_data.shape[0])\n",
    "df_bnb_data = df_bnb_data.query(\"evtTimeNS>0\")\n",
    "print(df_bnb_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf12413-12cc-47c9-b72b-e3c1706e9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge everything into a single peak with the run dependent corrections\n",
    "run = df_bnb_data[\"run\"].to_numpy()\n",
    "evtTimeNS = df_bnb_data[\"evtTimeNS\"].to_numpy()\n",
    "\n",
    "new_times = []\n",
    "\n",
    "for i in range(len(evtTimeNS)):\n",
    "    \n",
    "    gap=18.936\n",
    "    Shift=0\n",
    "    TThelp=0\n",
    "    if (run[i] >= 19500): Shift=2920.5 \n",
    "    elif (run[i] >= 17380): Shift=2916.0 \n",
    "    elif (run[i] >= 13697): Shift = 3147.3\n",
    "    elif (run[i] >= 10812): Shift = 3568.5 \n",
    "    elif (run[i] >= 8321): Shift = 3610.7\n",
    "    elif (run[i] >= 5800): Shift = 3164.4\n",
    "    elif (run[i] > 0 ): Shift = 3168.9\n",
    "    TThelp = evtTimeNS[i]-Shift+gap*0.5\n",
    "    TT_merged = -9999.\n",
    "\n",
    "    if(TThelp>=0 and TThelp<gap*81.0): \n",
    "        TT_merged=(TThelp-(int((TThelp)/gap))*gap)-gap*0.5\n",
    "        \n",
    "    new_times.append(TT_merged)\n",
    "\n",
    "df_bnb_data[\"merge_time\"] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d0d1d-91cc-449b-9320-bbafe8be83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the merged time peak\n",
    "\n",
    "nbins = 32\n",
    "\n",
    "data = df_bnb_data.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy()\n",
    "y,xbins = np.histogram(data,bins=nbins,range=(-9.42, 9.42))\n",
    "\n",
    "x = get_bin_centers(xbins)\n",
    "\n",
    "popt,pcov = curve_fit(gaus,x,y)\n",
    "print(\"data: Gaussian      mean:\",round(popt[1],4),\"  std:\",round(popt[2],4),\"  C:\",round(popt[3],4))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"BNB: numuCC selection\")\n",
    "plt.errorbar(get_bin_centers(xbins),y,yerr=np.sqrt(y),ms=8, lw=1,fmt='.',ecolor = 'black',color='black', capsize=2, capthick=1, label=\"Data\")\n",
    "plt.plot(x,gaus(x,*popt),color='darkgray',label='Data fit:'+'\\n'+f\"$\\mu$={round(popt[1],2)}, $\\sigma$={round(abs(popt[2]),2)}\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"Merged Time (ns)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2360c82-1caf-46ba-ae55-ca9fad94338b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "091db5df-4dec-4229-aa73-91cf776c0b76",
   "metadata": {},
   "source": [
    "## Plot the merged beam peak for the NuMI data and old MC\n",
    "No special steps for the data, this can be done the same for the old and new files. The overlay uses slightly different variables names and has the ns timing bug in the old MC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a7b37-5ca2-4108-bab7-f714e2d0474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the NuMI data\n",
    "file = uproot.open(numi_data_filename)\n",
    "pfeval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"evtTimeNS\",\"run\",\"subrun\",\"event\"], flatten=False)\n",
    "bdt_df = file[\"wcpselection\"][\"T_BDTvars\"].pandas.df([\"numu_score\"], flatten=False)\n",
    "df_numi_data = pd.concat([pfeval_df,bdt_df], axis=1, sort=False)\n",
    "del pfeval_df\n",
    "del bdt_df\n",
    "print(df_numi_data.shape[0])\n",
    "df_numi_data = df_numi_data.query(\"evtTimeNS>0\")\n",
    "print(df_numi_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b66283-1591-4187-a590-c73e2f1290b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge everything into a single peak\n",
    "evtTimeNS = df_numi_data[\"evtTimeNS\"].to_numpy()\n",
    "\n",
    "new_times = []\n",
    "\n",
    "for i in range(len(evtTimeNS)):\n",
    "\n",
    "    gap = 18.8305\n",
    "    Shift=0.8\n",
    "    TThelp=0\n",
    "\n",
    "    TThelp = evtTimeNS[i]-Shift+gap*0.5\n",
    "    TT_merged = -9999.\n",
    "\n",
    "    TT_merged=(TThelp-(int((TThelp)/gap))*gap)-gap*0.5\n",
    "        \n",
    "    new_times.append(TT_merged)\n",
    "\n",
    "df_numi_data[\"merge_time\"] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5f882-4727-4723-8294-bdcbe415dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the old NuMI MC\n",
    "file = uproot.open(old_numi_MC_filename)\n",
    "pfeval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"evtTimeNS_redk2nu\",\"run\",\"subrun\",\"event\"], flatten=False)\n",
    "#eval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"weight_cv\",\"weight_spline\"], flatten=False)\n",
    "bdt_df = file[\"wcpselection\"][\"T_BDTvars\"].pandas.df([\"numu_score\"], flatten=False)\n",
    "#df_numi_old_mc = pd.concat([pfeval_df,bdt_df,eval_df], axis=1, sort=False)\n",
    "df_numi_old_mc = pd.concat([pfeval_df,bdt_df], axis=1, sort=False)\n",
    "del pfeval_df\n",
    "del bdt_df\n",
    "print(df_numi_old_mc.shape[0])\n",
    "df_numi_old_mc = df_numi_old_mc.query(\"evtTimeNS_redk2nu>0\")\n",
    "print(df_numi_old_mc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91183e20-0c9a-4597-9d89-8f5ee5f626d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge everything into a single peak\n",
    "evtTimeNS = df_numi_old_mc[\"evtTimeNS_redk2nu\"].to_numpy()\n",
    "\n",
    "new_times = []\n",
    "\n",
    "for i in range(len(evtTimeNS)):\n",
    "\n",
    "    gap=18.831\n",
    "    Shift=2.8\n",
    "    TThelp=0\n",
    "\n",
    "    TThelp = evtTimeNS[i]-Shift+gap*0.5\n",
    "\n",
    "    TT_merged=(TThelp-(int((TThelp)/gap))*gap)-gap*0.5\n",
    "        \n",
    "    new_times.append(TT_merged)\n",
    "\n",
    "df_numi_old_mc[\"merge_time\"] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07a25e-6f41-49e5-a490-33a8fc1dd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 32\n",
    "\n",
    "emulate_ext = False\n",
    "\n",
    "data = df_numi_data.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy()\n",
    "y,xbins = np.histogram(data,bins=nbins,range=(-9.42, 9.42))\n",
    "\n",
    "overlay = df_numi_old_mc.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy()\n",
    "#weights = df_numi_old_mc.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"net_weight\"].to_numpy()\n",
    "weights = np.ones_like(df_numi_old_mc.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy())\n",
    "if(emulate_ext):\n",
    "    ext = np.random.uniform(-9.42, 9.42, size=int(len(overlay)*0.1))\n",
    "    overlay = np.concatenate((overlay,ext))\n",
    "weight = np.ones_like(overlay)*len(data)/len(overlay)\n",
    "y_overlay,xbins = np.histogram(overlay,bins=nbins,range=(-9.42, 9.42),weights=weight)\n",
    "\n",
    "x = get_bin_centers(xbins)\n",
    "popt_overlay,pcov = curve_fit(gaus,x,y_overlay/np.sum(y))\n",
    "print(\"overlay: Gaussian      mean:\",round(popt_overlay[1],4),\"  std:\",round(popt_overlay[2],4),\"  C:\",round(popt_overlay[3],4))\n",
    "\n",
    "popt,pcov = curve_fit(gaus,x,y/np.sum(y))\n",
    "print(\"data: Gaussian      mean:\",round(popt[1],4),\"  std:\",round(popt[2],4),\"  C:\",round(popt[3],4))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"NuMI: numuCC selection\")\n",
    "plt.errorbar(get_bin_centers(xbins),y/np.sum(y),yerr=np.sqrt(y)/np.sum(y),ms=8, lw=1,fmt='.',ecolor = 'black',color='black', capsize=2, capthick=1, label=\"Data\")\n",
    "plt.hist(overlay,bins=nbins,alpha=0.7,range=(-9.42, 9.42),label='Overlay',weights=weight/np.sum(y))\n",
    "plt.plot(x,gaus(x,*popt),color='darkgray',label='Data fit:'+'\\n'+f\"$\\mu$={round(popt[1],2)}, $\\sigma$={round(abs(popt[2]),2)}\")\n",
    "plt.plot(x,gaus(x,*popt_overlay),color='cyan',label='Overlay fit:'+'\\n'+f\"$\\mu$={round(popt_overlay[1],2)}, $\\sigma$={round(abs(popt_overlay[2]),2)}\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"Merged Time (ns)\")\n",
    "#plt.ylim(0,0.09)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58932a-cbbc-4c2d-81ab-7413621e5cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f4a2e-ca90-4a71-8b3c-185b2a50a179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f260dc4b-6fb8-47d2-9d97-6a97d720987c",
   "metadata": {},
   "source": [
    "## Helper functions and varibale definitions for re-calculating the ns time in MC without the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6867bef-f03b-471b-9953-06af3398ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants to do the recalibration\n",
    "\n",
    "#z plane location\n",
    "target_dir = [-0.46, -0.05, -0.885]\n",
    "min_a = -122.86902944472968\n",
    "min_b = 80.60659897339974\n",
    "min_c = 59.34119182916038\n",
    "\n",
    "#correction parameters, all zero for the MC\n",
    "f_ccnd1_a = 0\n",
    "f_ccnd1_b = 0\n",
    "f_ccnd2_a = 0\n",
    "f_ccnd2_b = 0\n",
    "f_ccnd3_a = 0\n",
    "f_ccnd3_b = 0\n",
    "f_ccnd3_c = 0\n",
    "f_ccnd3_d = 0\n",
    "f_ccnd4_a = 0\n",
    "f_ccnd4_b = 0\n",
    "f_ccnd4_1_a = 0\n",
    "f_ccnd4_1_b = 0\n",
    "f_ccnd4_2_a = 0\n",
    "f_ccnd4_2_b = 0\n",
    "dist_cut_x_cor = 99999\n",
    "\n",
    "RWM_offset = 0\n",
    "\n",
    "\n",
    "# pmt location\n",
    "PMT_location = [[-11.4545, -28.625, 990.356], [-11.4175, 27.607, 989.712],\n",
    "               [-11.7755, -56.514, 951.865], [-11.6415, 55.313, 951.861],\n",
    "               [-12.0585, -56.309, 911.939], [-11.8345, 55.822, 911.065],\n",
    "               [-12.1765, -0.722, 865.599], [-12.3045, -0.502, 796.208],\n",
    "               [-12.6045, -56.284, 751.905], [-12.5405, 55.625, 751.884],\n",
    "               [-12.6125, -56.408, 711.274], [-12.6615, 55.8, 711.073],\n",
    "               [-12.6245, -0.051, 664.203], [-12.6515, -0.549, 585.284],\n",
    "               [-12.8735, 55.822, 540.929], [-12.6205, -56.205, 540.616],\n",
    "               [-12.5945, -56.323, 500.221], [-12.9835, 55.771, 500.134],\n",
    "               [-12.6185, -0.875, 453.096], [-13.0855, -0.706, 373.839],\n",
    "               [-12.6485, -57.022, 328.341], [-13.1865, 54.693, 328.212],\n",
    "               [-13.4175, 54.646, 287.976], [-13.0075, -56.261, 287.639],\n",
    "               [-13.1505, -0.829, 242.014], [-13.4415, -0.303, 173.743],\n",
    "               [-13.3965, 55.249, 128.354], [-13.2784, -56.203, 128.18],\n",
    "               [-13.2375, -56.615, 87.8695], [-13.5415, 55.249, 87.7605],\n",
    "               [-13.4345, 27.431, 51.1015], [-13.1525, -28.576, 50.4745]]\n",
    "\n",
    "# Switch PMTs for the MC\n",
    "temp = PMT_location[31]\n",
    "PMT_location[31] = PMT_location[30]\n",
    "PMT_location[30] = PMT_location[29]\n",
    "PMT_location[29] = PMT_location[28]\n",
    "PMT_location[28] = PMT_location[27]\n",
    "PMT_location[27] = PMT_location[26]\n",
    "PMT_location[26] = temp\n",
    "\n",
    "\n",
    "#PMT timing ofsets, all zero for the MC\n",
    "offset = [1.03002, -5.18104, -2.11164, -5.99395, -1.25798, 0.633079, 2.87666, 2.21969, 0.885092, 2.35423, -1.63039, -1.83775, -0.859883, 3.4741, 1.84833, 1.58233, -2.71783, 0, 3.18776, 0.982666, 0.728438, 0.280592, -5.27068,-3.27857, -1.41196, 1.59643, 1.41425, -1.62682, -2.55772, 1.49136, -0.522791, 0.974533]\n",
    "offset = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "sol = 0.033356\n",
    "sol_Ar = 0.0746\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb63db2-115c-45c6-a50c-8610f5b89363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dE_dx(dqdx):\n",
    "    alpha = 0.93\n",
    "    beta = 0.212\n",
    "    dedx = (np.exp((dqdx) * 23.6e-6*beta/1.38/0.273) - alpha)/(beta/1.38/0.273)\n",
    "    return dedx\n",
    "    \n",
    "def get_dE_dx_range(R,pdg):\n",
    "    if pdg==22 or pdg==11 or pdg==2112: return 0\n",
    "    A = 8 \n",
    "    b = -0.37 \n",
    "    if pdg==2212:    \n",
    "        A = 17\n",
    "        b = -0.42\n",
    "    dedx = A*pow(R,b)\n",
    "    return dedx\n",
    "\n",
    "def get_T_range(R,pdg):\n",
    "    if pdg==22 or pdg==11 or pdg==2112: return 0\n",
    "    A = 8 \n",
    "    b = -0.37 \n",
    "    if pdg==2212:    \n",
    "        A = 17\n",
    "        b = -0.42\n",
    "    T = A/(b+1)*pow(R,b+1)\n",
    "    return T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa52d6-242f-4602-84a4-097f846c0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed to utilize PID for more realistic velocities.\n",
    "\n",
    "def get_time(momentum,startXYZT,endXYZT,pdg,mother_time):\n",
    "    dx_i = 0.5\n",
    "    dx = dx_i\n",
    "    time_position_points = []\n",
    "    length = np.sqrt( pow(startXYZT[0]-endXYZT[0],2) + pow(startXYZT[1]-endXYZT[1],2) + pow(startXYZT[2]-endXYZT[2],2) )\n",
    "    residual_range = length\n",
    "    x_pos = startXYZT[0]\n",
    "    y_pos = startXYZT[1]\n",
    "    z_pos = startXYZT[2]            \n",
    "    mass = 0\n",
    "    if pdg == 13: mass = 0.1057\n",
    "    if pdg == 2212: mass = 0.9397933 #Don't do neutrons, KE is not assigned well so just assume c\n",
    "    if pdg == 211: mass = 0.13982067  \n",
    "    KE = momentum[3] - mass\n",
    "    KE_R = momentum[3] - mass\n",
    "    v_i = np.nan_to_num(sol*1/np.sqrt( 1-pow(mass/(mass+KE),2) ),nan=sol)\n",
    "    if pdg==22 or pdg==2112 or pdg==11: v_i = sol\n",
    "    v = v_i\n",
    "    v_R = v_i\n",
    "    tPh = mother_time\n",
    "    tPh_R = mother_time\n",
    "    gamma = 0\n",
    "    DPh = np.sqrt( pow(startXYZT[0]-x_pos,2) + pow(startXYZT[1]-y_pos,2) + pow(startXYZT[2]-z_pos,2) )\n",
    "    tPh_alt = DPh*v_i+mother_time\n",
    "    while residual_range>=0:       \n",
    "        #save this point\n",
    "        time_position_points.append([x_pos,y_pos,z_pos,tPh])\n",
    "        #update for the next point\n",
    "        if residual_range<dx and dx==dx_i:\n",
    "            dx = residual_range*1.0000001\n",
    "        tPh += v*dx \n",
    "        dedx = get_dE_dx_range(residual_range,pdg)/1000\n",
    "        de = dedx*dx #automatically 0 for showers and neutrons, so just useing the \"alt\" treatment\n",
    "        KE = KE-de\n",
    "        v = np.nan_to_num(sol*1/np.sqrt( 1-pow(mass/(mass+KE),2) ),nan=sol)\n",
    "        if pdg==22 or pdg==2112 or pdg==11: v = sol\n",
    "            \n",
    "        tPh_R += v_R*dx\n",
    "        if(pdg==13 or pdg==2212 or abs(pdg)==211): KE_R = get_T_range(residual_range,pdg)/1000\n",
    "        v_R= np.nan_to_num(sol*1/np.sqrt( 1-pow(mass/(mass+KE_R),2) ),nan=sol)\n",
    "        if pdg==22 or pdg==2112 or pdg==11: v_R = sol\n",
    "\n",
    "        gamma+=(dx/length)\n",
    "        x_pos = startXYZT[0] + gamma*(endXYZT[0]-startXYZT[0])\n",
    "        y_pos = startXYZT[1] + gamma*(endXYZT[1]-startXYZT[1])\n",
    "        z_pos = startXYZT[2] + gamma*(endXYZT[2]-startXYZT[2])\n",
    "        residual_range = length - np.sqrt( pow(startXYZT[0]-x_pos,2) + pow(startXYZT[1]-y_pos,2) + pow(startXYZT[2]-z_pos,2)) \n",
    "        DPh = np.sqrt( pow(startXYZT[0]-x_pos,2) + pow(startXYZT[1]-y_pos,2) + pow(startXYZT[2]-z_pos,2) )\n",
    "        tPh_alt = DPh*v_i+mother_time\n",
    "        #print(length,residual_range,pdg,x_pos,y_pos,z_pos,v,KE,tPh)\n",
    "    return time_position_points\n",
    "\n",
    "def set_particle_propegation_times(momentum,startXYZT,endXYZT,ID,pdg,mother):\n",
    "\n",
    "    dx = 3\n",
    "\n",
    "    particle_times = {}\n",
    "\n",
    "    #first round, just do the primary\n",
    "    for part in range(len(pdg)):\n",
    "        # Check to see if we already added this particle\n",
    "        if ID[part] in particle_times: continue  \n",
    "        # Daughter particles get added after their mother to do the cummulative time right\n",
    "        if mother[part] != 0: continue\n",
    "\n",
    "        #print(ID[part],pdg[part])\n",
    "        time_position_points = get_time(momentum[part],startXYZT[part],endXYZT[part],pdg[part],0)       \n",
    "        particle_times.update({ID[part]:time_position_points})\n",
    "        #print(\"\")\n",
    "        \n",
    "        #check for daughters\n",
    "        daughters = []\n",
    "        \n",
    "        for daught_part in range(len(pdg)): \n",
    "            if mother[daught_part] == ID[part]: \n",
    "                if len(particle_times[ID[part]])>0: daughters.append([ID[daught_part],daught_part,particle_times[ID[part]][-1][3]])\n",
    "                else: daughters.append([ID[daught_part],daught_part,0])\n",
    "\n",
    "        while len(daughters) > 0 :\n",
    "            this_daughter = daughters[0]\n",
    "            daughters_id = this_daughter[0]\n",
    "            #double check we have not already added this particle\n",
    "            if daughters_id in particle_times: \n",
    "                daughters.remove(this_daughter)\n",
    "                continue \n",
    "            daughters_index = this_daughter[1]\n",
    "            mothers_time = this_daughter[2]\n",
    "            time_position_points = get_time(momentum[daughters_index],startXYZT[daughters_index],endXYZT[daughters_index],pdg[daughters_index],mothers_time)       \n",
    "            particle_times.update({daughters_id:time_position_points})  \n",
    "            #add the daughters of the daughters\n",
    "            for daught_part in range(len(pdg)): \n",
    "                if mother[daught_part] == daughters_id and not(ID[daught_part] in particle_times): \n",
    "                    if len(particle_times[daughters_id])>0: daughters.append([ID[daught_part],daught_part,particle_times[daughters_id][-1][3]])\n",
    "                    else: daughters.append([ID[daught_part],daught_part,0])\n",
    "            daughters.remove(this_daughter)\n",
    "            \n",
    "    return particle_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ab372-e572-41f5-b301-33dce5ea4467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a908a99-fec0-41db-bcec-1747a864e73b",
   "metadata": {},
   "source": [
    "## Plot the merged beam peak for the BNB data and new MC\n",
    "No special steps for the data, this can be done the same for the old and new files. The overlay needs to have the time recalculated without the calibration. Note that this is much slower than the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3f18c-ebee-4e72-a76c-0f708cc13101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the new BNB MC\n",
    "file = uproot.open(new_bnb_MC_filename)\n",
    "pfeval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"evtTimeNS\",\"run\",\"subrun\",\"event\"]+[\"reco_id\",\"reco_pdg\",\"reco_mother\",\"reco_startMomentum\",\"reco_startXYZT\",\"reco_endXYZT\",\"reco_nuvtxX\",\"reco_nuvtxY\",\"reco_nuvtxZ\",\"RWM_Time\",\"PMT_TimeProp\",\"PMT_Amp\",\"PMT_Time\",\"PMT_ID\",'cor_nu_deltatime'], flatten=False)\n",
    "bdt_df = file[\"wcpselection\"][\"T_BDTvars\"].pandas.df([\"numu_score\"], flatten=False)\n",
    "df_bnb_new_mc = pd.concat([pfeval_df,bdt_df], axis=1, sort=False)\n",
    "del pfeval_df\n",
    "del bdt_df\n",
    "print(df_bnb_new_mc.shape[0])\n",
    "df_bnb_new_mc = df_bnb_new_mc.query(\"evtTimeNS>0\")\n",
    "print(df_bnb_new_mc.shape[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b65df-9fce-45fb-a363-6ce6d58dfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMT_ID_list =  df_bnb_new_mc[\"PMT_ID\"].to_numpy()\n",
    "PMT_Time_list =  df_bnb_new_mc[\"PMT_Time\"].to_numpy()\n",
    "PMT_Amp_list =  df_bnb_new_mc[\"PMT_Amp\"].to_numpy()\n",
    "RWM_Time_list =  df_bnb_new_mc[\"RWM_Time\"].to_numpy()\n",
    "\n",
    "reco_nuvtxX_list =  df_bnb_new_mc[\"reco_nuvtxX\"].to_numpy()\n",
    "reco_nuvtxY_list =  df_bnb_new_mc[\"reco_nuvtxY\"].to_numpy()\n",
    "reco_nuvtxZ_list =  df_bnb_new_mc[\"reco_nuvtxZ\"].to_numpy()\n",
    "reco_id_list =  df_bnb_new_mc[\"reco_id\"].to_numpy()\n",
    "reco_pdg_list =  df_bnb_new_mc[\"reco_pdg\"].to_numpy()\n",
    "reco_startMomentum_list =  df_bnb_new_mc[\"reco_startMomentum\"].to_numpy()\n",
    "reco_startXYZT_list =  df_bnb_new_mc[\"reco_startXYZT\"].to_numpy()\n",
    "reco_endXYZT_list =  df_bnb_new_mc[\"reco_endXYZT\"].to_numpy()\n",
    "reco_mother_list =  df_bnb_new_mc[\"reco_mother\"].to_numpy()\n",
    "\n",
    "cor_nu_deltatime = df_bnb_new_mc[\"cor_nu_deltatime\"].to_numpy()\n",
    "\n",
    "full_t_list =  []\n",
    "full_new_x_list =  []\n",
    "full_new_y_list =  []\n",
    "full_new_z_list =  []\n",
    "full_part_list =  []\n",
    "full_pdg_list =  []\n",
    "full_mother_list =  []\n",
    "\n",
    "evtTimeNS_new = []\n",
    "evtTimeNS_nocor = []\n",
    "\n",
    "DLh_all = []\n",
    "DPh_all = []\n",
    "\n",
    "nu_tof = []\n",
    "\n",
    "TT3_array_all = []\n",
    "TT3_array_nocor_all = []\n",
    "\n",
    "timeProp_all = []\n",
    "\n",
    "Ph_Tot_all = []\n",
    "\n",
    "for event in tqdm(range(len(PMT_ID_list))):\n",
    "   \n",
    "    PMT_ID = PMT_ID_list[event]\n",
    "    PMT_Time = PMT_Time_list[event] \n",
    "    PMT_Amp = PMT_Amp_list[event]\n",
    "    RWM_Time = RWM_Time_list[event] \n",
    "\n",
    "    reco_nuvtxX = reco_nuvtxX_list[event]\n",
    "    reco_nuvtxY = reco_nuvtxY_list[event]\n",
    "    reco_nuvtxZ = reco_nuvtxZ_list[event]\n",
    "    reco_id = reco_id_list[event]\n",
    "    reco_pdg = reco_pdg_list[event]\n",
    "    reco_startMomentum = reco_startMomentum_list[event]\n",
    "    reco_startXYZT = reco_startXYZT_list[event]\n",
    "    reco_endXYZT = reco_endXYZT_list[event]\n",
    "    reco_mother = reco_mother_list[event]\n",
    "\n",
    "    full_t =  []\n",
    "    full_new_x =  []\n",
    "    full_new_y =  []\n",
    "    full_new_z =  []\n",
    "    full_part =  []\n",
    "    full_pdg =  []\n",
    "    full_mother =  []\n",
    "\n",
    "    \n",
    "    N_PMT = len(PMT_ID)\n",
    "     \n",
    "    if(N_PMT<3):\n",
    "\n",
    "        evtTimeNS_new.append(-99999)\n",
    "        evtTimeNS_nocor.append(-99999)\n",
    "        \n",
    "        nu_tof.append(-99999)\n",
    "\n",
    "        DPh_all.append([])\n",
    "        DLh_all.append([])\n",
    "        DPh_full_all.append([])\n",
    "        DLh_full_all.append([])\n",
    "\n",
    "        TT3_array_all.append([])\n",
    "        TT3_array_nocor_all.append([])\n",
    "\n",
    "        timeProp_all.append([])\n",
    "        timeProp_full_all.append([])\n",
    "\n",
    "        Ph_Tot_all.append(-99999)\n",
    "\n",
    "        full_t_list.append(full_t)\n",
    "        full_new_x_list.append(full_new_x)\n",
    "        full_new_y_list.append(full_new_y)\n",
    "        full_new_z_list.append(full_new_z)\n",
    "        full_part_list.append(full_part)\n",
    "        full_pdg_list.append(full_pdg)\n",
    "        full_mother_list.append(full_mother)  \n",
    "        \n",
    "        continue\n",
    "            \n",
    "    Ph_Tot = 0\n",
    "    \n",
    "    timeProp = []\n",
    "    timeProp_full = []\n",
    "\n",
    "    TT3_array = []\n",
    "    TT3_array_nocor = []\n",
    "    TT3_array_altcor = []\n",
    "    \n",
    "    ccnd1 = 0\n",
    "    ccnd2 = 0\n",
    "    ccnd3 = 0\n",
    "    ccnd4 = 0\n",
    "\n",
    "    DPh_event = []\n",
    "    DLh_event = []\n",
    "    \n",
    "    nuToF=reco_nuvtxZ*0.033356;\n",
    "    nu_tof.append(nuToF)\n",
    "\n",
    "    particle_times = set_particle_propegation_times(reco_startMomentum,reco_startXYZT,reco_endXYZT,reco_id,reco_pdg,reco_mother)\n",
    "   \n",
    "    for pmt in range(N_PMT):\n",
    "\n",
    "        pmt_id = PMT_ID[pmt]\n",
    "        \n",
    "        Ph_Tot=Ph_Tot+PMT_Amp[pmt]\n",
    "\n",
    "        tp=5000000000.0\n",
    "        \n",
    "        DPh_event.append(0)\n",
    "        DLh_event.append(0)\n",
    "        \n",
    "        for part in particle_times:\n",
    "            particle_time = particle_times[part]\n",
    "            pdg = 0\n",
    "            mother = 0\n",
    "            for i in range(len(reco_id)): \n",
    "                if reco_id[i]==part: \n",
    "                    pdg = reco_pdg[i]\n",
    "                    mother = reco_mother[i]\n",
    "                \n",
    "            for point in range(len(particle_time)):\n",
    "                particle_time_point = particle_time[point]\n",
    "                x_pos = particle_time_point[0]\n",
    "                y_pos = particle_time_point[1]\n",
    "                z_pos = particle_time_point[2]\n",
    "                DLh = np.sqrt( pow(PMT_location[pmt_id][0]-x_pos,2) + pow(PMT_location[pmt_id][1]-y_pos,2) + pow(PMT_location[pmt_id][2]-z_pos,2) )\n",
    "                DPh = np.sqrt( pow(reco_nuvtxX-x_pos,2) + pow(reco_nuvtxY-y_pos,2) + pow(reco_nuvtxZ-z_pos,2) ) \n",
    "                tPhelp = particle_time_point[3]+(DLh*sol_Ar)\n",
    "                if tPhelp<tp: \n",
    "                    tp=tPhelp\n",
    "                    DPh_event[pmt] = particle_time_point[3]\n",
    "                    DLh_event[pmt] = DLh\n",
    "                if pmt==0:\n",
    "                    full_new_x.append(particle_time_point[0])\n",
    "                    full_new_y.append(particle_time_point[1])\n",
    "                    full_new_z.append(particle_time_point[2])\n",
    "                    full_t.append(particle_time_point[3])\n",
    "                    full_part.append(part)\n",
    "                    full_pdg.append(pdg)\n",
    "                    full_mother.append(mother)\n",
    "                \n",
    "          \n",
    "        timeProp.append(tp)\n",
    "             \n",
    "    DPh_all.append(DPh_event)\n",
    "    DLh_all.append(DLh_event)    \n",
    "    timeProp_all.append(timeProp) \n",
    "    Ph_Tot_all.append(Ph_Tot)\n",
    "   \n",
    "    for pmt in range(N_PMT):\n",
    "        \n",
    "        pmt_id = PMT_ID[pmt]\n",
    "        \n",
    "        ccnd1 = timeProp[pmt]*(f_ccnd1_a)-(f_ccnd1_b)\n",
    "        ccnd2 = PMT_Amp[pmt]*(f_ccnd2_a)-(f_ccnd2_b)\n",
    "        if Ph_Tot>150: ccnd3=f_ccnd3_a-f_ccnd3_b*Ph_Tot+f_ccnd3_c*Ph_Tot*Ph_Tot \n",
    "        else: ccnd3=f_ccnd3_d\n",
    "        ccnd3 = timeProp[pmt]*(f_ccnd1_a)-(f_ccnd1_b)      \n",
    "        if reco_nuvtxX<dist_cut_x_cor: ccnd4 = reco_nuvtxX*(f_ccnd4_a)-(f_ccnd4_b)\n",
    "        else: ccnd4 = reco_nuvtxX*(f_ccnd4_2_a)-(f_ccnd4_2_b)\n",
    "        \n",
    "        TT3_array.append( PMT_Time[pmt] - RWM_Time_list[event] + RWM_offset - nuToF - timeProp[pmt] - offset[pmt_id] +ccnd1+ccnd2+ccnd3+ccnd4 +cor_nu_deltatime[event])\n",
    "        TT3_array_nocor.append( PMT_Time[pmt] - RWM_Time_list[event] + RWM_offset - nuToF - timeProp[pmt] - offset[pmt_id] +cor_nu_deltatime[event])\n",
    "       \n",
    "    Med_TT3 = np.median(TT3_array)\n",
    "    evtTimeNS_new.append(Med_TT3)\n",
    "    TT3_array_all.append(TT3_array)\n",
    "    \n",
    "    Med_TT3_nocor = np.median(TT3_array_nocor)\n",
    "    evtTimeNS_nocor.append(Med_TT3_nocor)\n",
    "    TT3_array_nocor_all.append(TT3_array_nocor)   \n",
    "    \n",
    "    full_t_list.append(full_t)\n",
    "    full_new_x_list.append(full_new_x)\n",
    "    full_new_y_list.append(full_new_y)\n",
    "    full_new_z_list.append(full_new_z)\n",
    "    full_part_list.append(full_part)\n",
    "    full_pdg_list.append(full_pdg)\n",
    "    full_mother_list.append(full_mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082cc2a-a454-42f4-af23-ae21db33d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bnb_new_mc[\"evtTimeNS_new\"] = evtTimeNS_new\n",
    "df_bnb_new_mc[\"evtTimeNS_nocor\"] = evtTimeNS_nocor\n",
    "\n",
    "df_bnb_new_mc[\"TT3_array\"] = TT3_array_all\n",
    "df_bnb_new_mc[\"TT3_array_nocor\"] = TT3_array_nocor_all\n",
    "\n",
    "df_bnb_new_mc[\"timeProp_all\"] = timeProp_all\n",
    "\n",
    "df_bnb_new_mc[\"nu_tof\"] = nu_tof\n",
    "\n",
    "df_bnb_new_mc[\"DPh_all\"] = DPh_all\n",
    "df_bnb_new_mc[\"DLh_all\"] = DLh_all\n",
    "\n",
    "df_bnb_new_mc[\"Ph_Tot\"] = Ph_Tot_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7c485-7c9c-42aa-b78b-6b54eef4dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge everything into a single peak\n",
    "evtTimeNS = df_bnb_new_mc[\"evtTimeNS_new\"].to_numpy()\n",
    "new_times = []\n",
    "\n",
    "for i in range(len(evtTimeNS)):\n",
    "\n",
    "    if evtTimeNS[i]<0: \n",
    "        new_times.append(-9999)\n",
    "        continue\n",
    "\n",
    "    gap = 18.831\n",
    "    Shift=14.1\n",
    "    TThelp=0\n",
    "\n",
    "    TThelp = evtTimeNS[i]-Shift+gap*0.5\n",
    "    TT_merged = -9999.\n",
    "\n",
    "    TT_merged=(TThelp-(int((TThelp)/gap))*gap)-gap*0.5\n",
    "        \n",
    "    new_times.append(TT_merged)\n",
    "\n",
    "df_bnb_new_mc[\"merge_time_new\"] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d15a2-f35e-4c7f-a419-fa0aa7da38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 32\n",
    "\n",
    "emulate_ext = False\n",
    "\n",
    "data = df_bnb_data.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy()\n",
    "y,xbins = np.histogram(data,bins=nbins,range=(-9.42, 9.42))\n",
    "\n",
    "overlay = df_bnb_new_mc.query(\"merge_time_new>-9.42 and merge_time_new<9.42 and numu_score>0.9\")[\"merge_time_new\"].to_numpy()\n",
    "weights = np.ones_like(df_bnb_new_mc.query(\"merge_time_new>-9.42 and merge_time_new<9.42 and numu_score>0.9\")[\"merge_time_new\"].to_numpy())\n",
    "if(emulate_ext):\n",
    "    ext = np.random.uniform(-9.42, 9.42, size=int(len(overlay)*0.1))\n",
    "    overlay = np.concatenate((overlay,ext))\n",
    "weight = np.ones_like(overlay)*len(data)/len(overlay)\n",
    "y_overlay,xbins = np.histogram(overlay,bins=nbins,range=(-9.42, 9.42),weights=weight)\n",
    "\n",
    "x = get_bin_centers(xbins)\n",
    "\n",
    "popt_overlay,pcov = curve_fit(gaus,x,y_overlay/np.sum(y))\n",
    "print(\"overlay: Gaussian      mean:\",round(popt_overlay[1],4),\"  std:\",round(popt_overlay[2],4),\"  C:\",round(popt_overlay[3],4))\n",
    "\n",
    "popt,pcov = curve_fit(gaus,x,y/np.sum(y))\n",
    "print(\"data: Gaussian      mean:\",round(popt[1],4),\"  std:\",round(popt[2],4),\"  C:\",round(popt[3],4))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"BNB: numuCC selection\")\n",
    "plt.errorbar(get_bin_centers(xbins),y/np.sum(y),yerr=np.sqrt(y)/np.sum(y),ms=8, lw=1,fmt='.',ecolor = 'black',color='black', capsize=2, capthick=1, label=\"Data\")\n",
    "plt.hist(overlay,bins=nbins,alpha=0.7,range=(-9.42, 9.42),label='Overlay',weights=weight/np.sum(y))\n",
    "plt.plot(x,gaus(x,*popt),color='darkgray',label='Data fit:'+'\\n'+f\"$\\mu$={round(popt[1],2)}, $\\sigma$={round(abs(popt[2]),2)}\")\n",
    "plt.plot(x,gaus(x,*popt_overlay),color='cyan',label='Overlay fit:'+'\\n'+f\"$\\mu$={round(popt_overlay[1],2)}, $\\sigma$={round(abs(popt_overlay[2]),2)}\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"Merged Time (ns)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851ea6d-308e-4259-baa5-74a585f79290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eae511f-3f29-4184-af8a-316cff7fc656",
   "metadata": {},
   "source": [
    "## Plot the merged beam peak for the NuMI data and new MC\n",
    "No special steps for the data, this can be done the same for the old and new files. The overlay needs to have the time recalculated without the calibration. Note that this is much slower than the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1ab37-e9f5-4126-b2f8-04a7618a18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the new NuMI MC\n",
    "file = uproot.open(new_numi_MC_filename)\n",
    "pfeval_df = file[\"wcpselection\"][\"T_PFeval\"].pandas.df([\"evtTimeNS\",\"run\",\"subrun\",\"event\"]+[\"reco_id\",\"reco_pdg\",\"reco_mother\",\"reco_startMomentum\",\"reco_startXYZT\",\"reco_endXYZT\",\"reco_nuvtxX\",\"reco_nuvtxY\",\"reco_nuvtxZ\",\"RWM_Time\",\"PMT_TimeProp\",\"PMT_Amp\",\"PMT_Time\",\"PMT_ID\",'cor_nu_deltatime'], flatten=False)\n",
    "bdt_df = file[\"wcpselection\"][\"T_BDTvars\"].pandas.df([\"numu_score\"], flatten=False)\n",
    "df_numi_new_mc = pd.concat([pfeval_df,bdt_df], axis=1, sort=False)\n",
    "del pfeval_df\n",
    "del bdt_df\n",
    "print(df_numi_new_mc.shape[0])\n",
    "df_numi_new_mc = df_numi_new_mc.query(\"evtTimeNS>0\")\n",
    "print(df_numi_new_mc.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1cfac-ce9b-4d1c-96c4-c51ec41cf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "PMT_ID_list =  df_numi_new_mc[\"PMT_ID\"].to_numpy()\n",
    "PMT_Time_list =  df_numi_new_mc[\"PMT_Time\"].to_numpy()\n",
    "PMT_Amp_list =  df_numi_new_mc[\"PMT_Amp\"].to_numpy()\n",
    "RWM_Time_list =  df_numi_new_mc[\"RWM_Time\"].to_numpy()\n",
    "\n",
    "reco_nuvtxX_list =  df_numi_new_mc[\"reco_nuvtxX\"].to_numpy()\n",
    "reco_nuvtxY_list =  df_numi_new_mc[\"reco_nuvtxY\"].to_numpy()\n",
    "reco_nuvtxZ_list =  df_numi_new_mc[\"reco_nuvtxZ\"].to_numpy()\n",
    "reco_id_list =  df_numi_new_mc[\"reco_id\"].to_numpy()\n",
    "reco_pdg_list =  df_numi_new_mc[\"reco_pdg\"].to_numpy()\n",
    "reco_startMomentum_list =  df_numi_new_mc[\"reco_startMomentum\"].to_numpy()\n",
    "reco_startXYZT_list =  df_numi_new_mc[\"reco_startXYZT\"].to_numpy()\n",
    "reco_endXYZT_list =  df_numi_new_mc[\"reco_endXYZT\"].to_numpy()\n",
    "reco_mother_list =  df_numi_new_mc[\"reco_mother\"].to_numpy()\n",
    "\n",
    "cor_nu_deltatime = df_numi_new_mc[\"cor_nu_deltatime\"].to_numpy()\n",
    "\n",
    "\n",
    "full_t_list =  []\n",
    "full_new_x_list =  []\n",
    "full_new_y_list =  []\n",
    "full_new_z_list =  []\n",
    "full_part_list =  []\n",
    "full_pdg_list =  []\n",
    "full_mother_list =  []\n",
    "\n",
    "evtTimeNS_new = []\n",
    "evtTimeNS_nocor = []\n",
    "\n",
    "DLh_all = []\n",
    "DPh_all = []\n",
    "\n",
    "nu_tof = []\n",
    "\n",
    "TT3_array_all = []\n",
    "TT3_array_nocor_all = []\n",
    "\n",
    "timeProp_all = []\n",
    "\n",
    "Ph_Tot_all = []\n",
    "\n",
    "for event in tqdm(range(len(PMT_ID_list))):\n",
    "   \n",
    "    PMT_ID = PMT_ID_list[event]\n",
    "    PMT_Time = PMT_Time_list[event] \n",
    "    PMT_Amp = PMT_Amp_list[event]\n",
    "    RWM_Time = RWM_Time_list[event] \n",
    "\n",
    "    reco_nuvtxX = reco_nuvtxX_list[event]\n",
    "    reco_nuvtxY = reco_nuvtxY_list[event]\n",
    "    reco_nuvtxZ = reco_nuvtxZ_list[event]\n",
    "    reco_id = reco_id_list[event]\n",
    "    reco_pdg = reco_pdg_list[event]\n",
    "    reco_startMomentum = reco_startMomentum_list[event]\n",
    "    reco_startXYZT = reco_startXYZT_list[event]\n",
    "    reco_endXYZT = reco_endXYZT_list[event]\n",
    "    reco_mother = reco_mother_list[event]\n",
    "\n",
    "    full_t =  []\n",
    "    full_new_x =  []\n",
    "    full_new_y =  []\n",
    "    full_new_z =  []\n",
    "    full_part =  []\n",
    "    full_pdg =  []\n",
    "    full_mother =  []\n",
    "\n",
    "    \n",
    "    N_PMT = len(PMT_ID)\n",
    "     \n",
    "    if(N_PMT<3):\n",
    "\n",
    "        evtTimeNS_new.append(-99999)\n",
    "        evtTimeNS_nocor.append(-99999)\n",
    "        \n",
    "        nu_tof.append(-99999)\n",
    "\n",
    "        DPh_all.append([])\n",
    "        DLh_all.append([])\n",
    "        DPh_full_all.append([])\n",
    "        DLh_full_all.append([])\n",
    "\n",
    "        TT3_array_all.append([])\n",
    "        TT3_array_nocor_all.append([])\n",
    "\n",
    "        timeProp_all.append([])\n",
    "        timeProp_full_all.append([])\n",
    "\n",
    "        Ph_Tot_all.append(-99999)\n",
    "\n",
    "        full_t_list.append(full_t)\n",
    "        full_new_x_list.append(full_new_x)\n",
    "        full_new_y_list.append(full_new_y)\n",
    "        full_new_z_list.append(full_new_z)\n",
    "        full_part_list.append(full_part)\n",
    "        full_pdg_list.append(full_pdg)\n",
    "        full_mother_list.append(full_mother)  \n",
    "        \n",
    "        continue\n",
    "            \n",
    "    Ph_Tot = 0\n",
    "    \n",
    "    timeProp = []\n",
    "    timeProp_full = []\n",
    "\n",
    "    TT3_array = []\n",
    "    TT3_array_nocor = []\n",
    "    TT3_array_altcor = []\n",
    "    \n",
    "    ccnd1 = 0\n",
    "    ccnd2 = 0\n",
    "    ccnd3 = 0\n",
    "    ccnd4 = 0\n",
    "\n",
    "    DPh_event = []\n",
    "    DLh_event = []\n",
    "    \n",
    "    dist = ( (min_a-reco_nuvtxX)*target_dir[0] + (min_b-reco_nuvtxY)*target_dir[1] + (min_c-reco_nuvtxZ)*target_dir[2] ) / np.sqrt(target_dir[0]*target_dir[0] + target_dir[1]*target_dir[1] + target_dir[2]*target_dir[2] )\n",
    "    nuToF=dist*0.033356;\n",
    "    nu_tof.append(nuToF)\n",
    "\n",
    "    particle_times = set_particle_propegation_times(reco_startMomentum,reco_startXYZT,reco_endXYZT,reco_id,reco_pdg,reco_mother)\n",
    "   \n",
    "    for pmt in range(N_PMT):\n",
    "\n",
    "        pmt_id = PMT_ID[pmt]\n",
    "        \n",
    "        Ph_Tot=Ph_Tot+PMT_Amp[pmt]\n",
    "\n",
    "        tp=5000000000.0\n",
    "        \n",
    "        DPh_event.append(0)\n",
    "        DLh_event.append(0)\n",
    "        \n",
    "        for part in particle_times:\n",
    "            particle_time = particle_times[part]\n",
    "            pdg = 0\n",
    "            mother = 0\n",
    "            for i in range(len(reco_id)): \n",
    "                if reco_id[i]==part: \n",
    "                    pdg = reco_pdg[i]\n",
    "                    mother = reco_mother[i]\n",
    "                \n",
    "            for point in range(len(particle_time)):\n",
    "                particle_time_point = particle_time[point]\n",
    "                x_pos = particle_time_point[0]\n",
    "                y_pos = particle_time_point[1]\n",
    "                z_pos = particle_time_point[2]\n",
    "                DLh = np.sqrt( pow(PMT_location[pmt_id][0]-x_pos,2) + pow(PMT_location[pmt_id][1]-y_pos,2) + pow(PMT_location[pmt_id][2]-z_pos,2) )\n",
    "                DPh = np.sqrt( pow(reco_nuvtxX-x_pos,2) + pow(reco_nuvtxY-y_pos,2) + pow(reco_nuvtxZ-z_pos,2) ) \n",
    "                tPhelp = particle_time_point[3]+(DLh*sol_Ar)\n",
    "                if tPhelp<tp: \n",
    "                    tp=tPhelp\n",
    "                    DPh_event[pmt] = particle_time_point[3]\n",
    "                    DLh_event[pmt] = DLh\n",
    "                if pmt==0:\n",
    "                    full_new_x.append(particle_time_point[0])\n",
    "                    full_new_y.append(particle_time_point[1])\n",
    "                    full_new_z.append(particle_time_point[2])\n",
    "                    full_t.append(particle_time_point[3])\n",
    "                    full_part.append(part)\n",
    "                    full_pdg.append(pdg)\n",
    "                    full_mother.append(mother)\n",
    "                \n",
    "          \n",
    "        timeProp.append(tp)\n",
    "             \n",
    "    DPh_all.append(DPh_event)\n",
    "    DLh_all.append(DLh_event)    \n",
    "    timeProp_all.append(timeProp) \n",
    "    Ph_Tot_all.append(Ph_Tot)\n",
    "   \n",
    "    for pmt in range(N_PMT):\n",
    "        \n",
    "        pmt_id = PMT_ID[pmt]\n",
    "        \n",
    "        ccnd1 = timeProp[pmt]*(f_ccnd1_a)-(f_ccnd1_b)\n",
    "        ccnd2 = PMT_Amp[pmt]*(f_ccnd2_a)-(f_ccnd2_b)\n",
    "        if Ph_Tot>150: ccnd3=f_ccnd3_a-f_ccnd3_b*Ph_Tot+f_ccnd3_c*Ph_Tot*Ph_Tot \n",
    "        else: ccnd3=f_ccnd3_d\n",
    "        ccnd3 = timeProp[pmt]*(f_ccnd1_a)-(f_ccnd1_b)      \n",
    "        if reco_nuvtxX<dist_cut_x_cor: ccnd4 = reco_nuvtxX*(f_ccnd4_a)-(f_ccnd4_b)\n",
    "        else: ccnd4 = reco_nuvtxX*(f_ccnd4_2_a)-(f_ccnd4_2_b)\n",
    "        \n",
    "        TT3_array.append( PMT_Time[pmt] - RWM_Time_list[event] + RWM_offset - nuToF - timeProp[pmt] - offset[pmt_id] +ccnd1+ccnd2+ccnd3+ccnd4 +cor_nu_deltatime[event])\n",
    "        TT3_array_nocor.append( PMT_Time[pmt] - RWM_Time_list[event] + RWM_offset - nuToF - timeProp[pmt] - offset[pmt_id] +cor_nu_deltatime[event])\n",
    "       \n",
    "    Med_TT3 = np.median(TT3_array)\n",
    "    evtTimeNS_new.append(Med_TT3)\n",
    "    TT3_array_all.append(TT3_array)\n",
    "    \n",
    "    Med_TT3_nocor = np.median(TT3_array_nocor)\n",
    "    evtTimeNS_nocor.append(Med_TT3_nocor)\n",
    "    TT3_array_nocor_all.append(TT3_array_nocor)   \n",
    "    \n",
    "    full_t_list.append(full_t)\n",
    "    full_new_x_list.append(full_new_x)\n",
    "    full_new_y_list.append(full_new_y)\n",
    "    full_new_z_list.append(full_new_z)\n",
    "    full_part_list.append(full_part)\n",
    "    full_pdg_list.append(full_pdg)\n",
    "    full_mother_list.append(full_mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f81a0-95f6-4e41-acd1-174c64db39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge everything into a single peak\n",
    "evtTimeNS = df_numi_new_mc[\"evtTimeNS_new\"].to_numpy()\n",
    "\n",
    "new_times = []\n",
    "\n",
    "for i in range(len(evtTimeNS)):\n",
    "\n",
    "    if evtTimeNS[i]<0: \n",
    "        new_times.append(-9999)\n",
    "        continue\n",
    "    \n",
    "    gap=18.831\n",
    "    Shift=0\n",
    "    TThelp=0\n",
    "\n",
    "    TThelp = evtTimeNS[i]-Shift+gap*0.5\n",
    "    TT_merged = -9999.\n",
    "\n",
    "    if(TThelp>=0 and TThelp<gap*81.0): \n",
    "        TT_merged=(TThelp-(int((TThelp)/gap))*gap)-gap*0.5\n",
    "        \n",
    "    new_times.append(TT_merged)\n",
    "\n",
    "df_numi_new_mc[\"merge_time_new\"] = new_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ee0f9-9d6a-4e4e-9c1e-01879e7437fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 32\n",
    "\n",
    "emulate_ext = False\n",
    "\n",
    "data = df_numi_data.query(\"merge_time>-9.42 and merge_time<9.42 and numu_score>0.9\")[\"merge_time\"].to_numpy()\n",
    "y,xbins = np.histogram(data,bins=nbins,range=(-9.42, 9.42))\n",
    "\n",
    "overlay = df_numi_new_mc.query(\"merge_time_new>-9.42 and merge_time_new<9.42 and numu_score>0.9\")[\"merge_time_new\"].to_numpy()\n",
    "weights = df_numi_new_mc.query(\"merge_time_new>-9.42 and merge_time_new<9.42 and numu_score>0.9\")[\"net_weight\"].to_numpy()\n",
    "if(emulate_ext):\n",
    "    ext = np.random.uniform(-9.42, 9.42, size=int(len(overlay)*0.1))\n",
    "    overlay = np.concatenate((overlay,ext))\n",
    "weight = np.ones_like(overlay)*len(data)/len(overlay)\n",
    "y_overlay,xbins = np.histogram(overlay,bins=nbins,range=(-9.42, 9.42),weights=weight)\n",
    "\n",
    "x = get_bin_centers(xbins)\n",
    "\n",
    "popt_overlay,pcov = curve_fit(gaus,x,y_overlay/np.sum(y))\n",
    "print(\"overlay: Gaussian      mean:\",round(popt_overlay[1],4),\"  std:\",round(popt_overlay[2],4),\"  C:\",round(popt_overlay[3],4))\n",
    "\n",
    "popt,pcov = curve_fit(gaus,x,y/np.sum(y))\n",
    "print(\"data: Gaussian      mean:\",round(popt[1],4),\"  std:\",round(popt[2],4),\"  C:\",round(popt[3],4))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"NuMI: numuCC selection\")\n",
    "plt.errorbar(get_bin_centers(xbins),y/np.sum(y),yerr=np.sqrt(y)/np.sum(y),ms=8, lw=1,fmt='.',ecolor = 'black',color='black', capsize=2, capthick=1, label=\"Data\")\n",
    "plt.hist(overlay,bins=nbins,alpha=0.7,range=(-9.42, 9.42),label='Overlay',weights=weight/np.sum(y))\n",
    "plt.plot(x,gaus(x,*popt),color='darkgray',label='Data fit:'+'\\n'+f\"$\\mu$={round(popt[1],2)}, $\\sigma$={round(abs(popt[2]),2)}\")\n",
    "plt.plot(x,gaus(x,*popt_overlay),color='cyan',label='Overlay fit:'+'\\n'+f\"$\\mu$={round(popt_overlay[1],2)}, $\\sigma$={round(abs(popt_overlay[2]),2)}\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"Merged Time (ns)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363babdd-4145-4226-a595-526f6bc2215b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
